# AI Article Rectification Challenge

## Problem Statement

Large Language Models (LLMs) are excellent at fluency and structure but notorious for factual inconsistency. We have a dataset of **104 articles** generated by an AI agent based on ground-truth source documents. While the writing style is high-quality, the content contains subtle but critical inaccuracies.

The source article used to generate each article is available in the `source_articles/` folder, and the respective article generated by the AI agent is available in the `ai_generated_articles/` folder.

**Your Mission**: Develop an automated **AI Editor** pipeline. Your system must intake an imperfect AI-generated article, cross-reference it with the original Source Text, and output a rectified version.

### The Core Challenge: Accuracy vs. Preservation

The difficulty of this challenge lies in the competing constraints:

1. **Factuality**: You must fix 100% of the hallucinations and errors.
2. **Preservation**: You must barely touch the text.

Most LLMs, when asked to "fix" a text, will rewrite the entire paragraph, changing the tone, sentence structure, and phrasing. **This is strictly forbidden**. Your system must act like a surgeon, not an author. It should excise the error and stitch in the fact without leaving a scar on the surrounding prose.

### Example

**AI-Generated Content (with error):**
> "The Pacific Ocean is the largest ocean, covering approximately 63 million square kilometers of Earth's surface."

**Error:** The Pacific Ocean covers approximately 165 million square kilometers, not 63 million.

**Version 1:**
> "The Pacific Ocean is the biggest ocean, covering approximately 165 million square kilometers of Earth's surface."

❌ **Why it's wrong:** The error is fixed, but other correct parts were unnecessarily changed ("largest" → "biggest").

**Version 2:**
> "The Pacific Ocean is the largest ocean, covering approximately 165 million square kilometers of Earth's surface."

✅ **Why it's correct:** Only the error was fixed ("63 million" → "165 million"), with no other modifications to the already-correct content.

### The Dataset

You are provided with three directories:

- `source_articles/`: This folder contains the source article used to generate each article.
- `ai_generated_articles/`: The article containing errors.
- `rectified_articles/`: 10 examples showing exactly how a human editor corrected the AI text. Use these to understand the expected output format and "strictness."

## LLM API Access & Budget Constraints

We will provide you with an LLM API key to develop and deploy your **AI Editor** pipeline.

**The Budget**: You have a strict hard limit of $3.00 USD for `openai/gpt-oss-120b` model.

Based on current model pricing, this equates to approximately **10 million tokens**. This budget is sufficient, but finite. You must manage this resource effectively to cover your development, testing, and internal validation.

### Monitoring Your Spend

You are responsible for tracking your usage. We have provided a `budget_checker.py` module to help you view your remaining balance in real-time.

```bash
python budget_checker.py
```

## Expectations & Deliverables

To ensure your submission can be automatically graded, you must adhere to the following strict operational requirements.

### 1. Reproducibility via Command Line

Your system must be fully executable via the provided entry point. We will grade your submission by running exactly **one** command:

```bash
python3 rectifier.py rectify-all
```

**Requirement**: When this command finishes, the `rectified_articles/` folder must contain **all 104** rectified text files. If your code requires manual intervention, notebook cells, or complex setups to generate the files, it will be marked as incomplete.

### 2. Dependency Management

You must ensure the environment is reproducible.

- **Update** `requirements.txt`: If you use any libraries not already listed (e.g., spacy, nltk, langchain, pydantic), you **must** add them to `requirements.txt`.
- **Standard Libraries**: Do not rely on libraries that require complex system-level installs (like specific C++ compilers) unless absolutely necessary and documented.

### 3. Documentation of Third-Party Services

If your solution relies on any external APIs or third-party services (other than the provided LiteLLM key), you must strictly document them in your `README.md`.

- Example: If you use a vector database (like Pinecone) or a search API, you must explain how to set it up and provide the necessary environment variable keys in a separate `env.example` file.

### 4. Output Integrity

Your code must save the final output to the rectified_articles/ directory.

- **File Naming**: The filenames must match the original IDs (e.g., article_001.txt).
- **Content**: The files should contain only the rectified article text. Do not include markdown code blocks (like ```) or JSON artifacts in the final text files.

## Getting Started

### Setup

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Configure LLM credentials in `.env` file

3. Understand the codebase structure by reviewing `rectifier.py`

### Running the Demo

```bash
# Check remaining budget
python budget_checker.py

# Check budget with usage guide
python budget_checker.py --guide

# Test on first 16 articles
python rectifier.py test

# Test on custom count
python rectifier.py test --count 5

# Process all 104 articles
python rectifier.py rectify-all
```

## Building Your Solution

### Integration Point

Plug your rectification logic into `rectifier.py` at line 51:

```python
def rectify_article(article_id: str):
    ai_generated_content = get_ai_generated_article(article_id)
    
    # PLUG YOUR CUSTOM RECTIFIER HERE
    rectified_content = run(ai_generated_content)
    ###################################
    
    save_rectified_article(article_id, rectified_content)
    return rectified_content
```

### Implementation Freedom

You have complete flexibility to design your system:
- Replace the `run()` function or build a new architecture
- Access source articles via `get_article_mapping(article_id)`
- Create multi-file systems with any structure
- Use multiple LLM calls, validation layers, or confidence scoring
- Implement any approach that effectively solves the problem

## What to Submit

1. **Complete source code** with clear documentation
2. **Updated `requirements.txt`** with all dependencies
3. Documentation: A concise README explaining your approach and system architecture.

## ⚠️ Submission Verification (Critical)

We will evaluate your submission by running the following command on our environment:

```bash
python rectifier.py rectify-all
```

**If this command fails, crashes, or requires manual intervention, your submission will be disqualified.**

Ensure that your code handles exceptions gracefully and that your logic is robust enough to process the entire batch without stopping.

## Success Tips

- **Design your evaluation metric first**—use it to iteratively improve your system
- **Start small** (5-10 articles), validate, then scale up
- **Use the 10 reference examples** in `rectified_articles/` to validate your approach
- **Optimize token usage**—you have a limited budget (~10M tokens for development)
- **Document your reasoning** for key design decisions


---

**Good luck! We're excited to see your approach to this challenge.**

