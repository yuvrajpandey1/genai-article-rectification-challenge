# AI Article Rectification Challenge

## Problem Statement

An AI agent has generated 104 articles based on source material. While many parts are accurate, some contain critical errors such as hallucinations, inference errors, and many more.

The source article used to generate each article is available in the `source_articles/` folder, and the respective article generated by the AI agent is available in the `ai_generated_articles/` folder.

**Your Task:** Build a system that identifies and corrects the errors in the AI-generated articles while preserving accurate content and minimizing unnecessary changes. We have provided 10 rectified articles for reference in the `rectified_articles/` folder.

**Important:** Your rectifier must make __minimal changes__, only correcting errors while preserving all accurate content. You will lose points for modifying any part of the article that was already correct. See the example below to understand what constitutes __minimal changes__.

### Example

**AI-Generated Content (with error):**
> "The Pacific Ocean is the largest ocean, covering approximately 63 million square kilometers of Earth's surface."

**Error:** The Pacific Ocean covers approximately 165 million square kilometers, not 63 million.

**Version 1:**
> "The Pacific Ocean is the biggest ocean, covering approximately 165 million square kilometers of Earth's surface."

❌ **Why it's wrong:** The error is fixed, but other correct parts were unnecessarily changed ("largest" → "biggest").

**Version 2:**
> "The Pacific Ocean is the largest ocean, covering approximately 165 million square kilometers of Earth's surface."

✅ **Why it's correct:** Only the error was fixed ("63 million" → "165 million"), with no other modifications to the already-correct content.

## LLM API Access & Budget Constraints

We will provide you with an LLM API key to develop and deploy your rectification system.

**The Budget**: You have a strict hard limit of $3.00 USD.

Based on current model pricing, this equates to approximately **10 million tokens**. This budget is sufficient, but finite. You must manage this resource effectively to cover the entire lifecycle of the project:

- Development & Iteration: Testing your prompts and logic on small subsets.
- Evaluation: Running your internal validation steps.
- Final Production Run: Generating the final rectified versions for all 104 articles.

### Monitoring Your Spend

You are responsible for tracking your usage. We have provided a `budget_checker.py` module to help you view your remaining balance in real-time.

Tip: We recommend calculating the average cost per article during your testing phase to ensure you reserve enough budget (approx. $1.00 - $1.50) for the final batch processing of all articles.

## Getting Started

### Setup

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Configure LLM credentials in `.env` file

3. Understand the codebase structure by reviewing `rectifier.py`

### Running the Demo

```bash
# Check remaining budget
python budget_checker.py

# Check budget with usage guide
python budget_checker.py --guide

# Test on first 16 articles
python rectifier.py test

# Test on custom count
python rectifier.py test --count 5

# Process all 104 articles
python rectifier.py rectify-all
```

## Building Your Solution

### Integration Point

Plug your rectification logic into `rectifier.py` at line 51:

```python
def rectify_article(article_id: str):
    ai_generated_content = get_ai_generated_article(article_id)
    
    # PLUG YOUR CUSTOM RECTIFIER HERE
    rectified_content = run(ai_generated_content)
    ###################################
    
    save_rectified_article(article_id, rectified_content)
    return rectified_content
```

### Implementation Freedom

You have complete flexibility to design your system:
- Replace the `run()` function or build a new architecture
- Access source articles via `get_article_mapping(article_id)`
- Create multi-file systems with any structure
- Use multiple LLM calls, validation layers, or confidence scoring
- Implement any approach that effectively solves the problem

## What to Submit

1. **Complete source code** with clear documentation
2. **All 100 rectified articles** in `rectified_articles/` (mandatory)
3. **Documentation** explaining:
   - How to run your system
   - Your evaluation metric design and validation approach
   - High-level architecture and design decisions
4. **Updated `requirements.txt`** with all dependencies

## Success Tips

- **Design your evaluation metric first**—use it to iteratively improve your system
- **Start small** (5-10 articles), validate, then scale up
- **Use the 10 reference examples** in `rectified_articles/` to validate your approach
- **Optimize token usage**—you have a limited budget (~10M tokens for development)
- **Document your reasoning** for key design decisions


---

**Good luck! We're excited to see your approach to this challenge.**

